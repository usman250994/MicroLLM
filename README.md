
# My First Micro LLM: Step-by-Step Guide

>This project is a minimal, easy-to-follow example of building a retrieval-augmented question-answering (QA) system using OpenAI, LangChain, and Chroma vector database in Python. It is designed for beginners and can be extended for more advanced use cases.

---

## How It Works: Code Walkthrough

Below is a line-by-line explanation of the main logic (starting from line 8 in `main.py`).

### 1. Load Environment Variables
```python
load_dotenv()
```
Loads environment variables from a `.env` file. This is where you should store your OpenAI API key securely.

### 2. Read the OpenAI API Key
```python
api_key = os.getenv("OPENAI_API_KEY")
```
Fetches your OpenAI API key from the environment variables.

### 3. Prepare Documents
```python
documents = [
		Document(page_content="FastAPI is a Python web framework for building APIs quickly."),
		Document(page_content="Python is a programming language used for web, ML, and scripting.")
]
```
Wraps your text data as `Document` objects. Each document represents a chunk of information that can be searched and retrieved later.

### 4. Create Embeddings and Vector Database
```python
embeddings = OpenAIEmbeddings(openai_api_key=api_key)
vectordb = Chroma.from_documents(documents, embeddings)
```
- `OpenAIEmbeddings` converts your documents into numerical vectors using OpenAI's embedding model.
- `Chroma.from_documents` creates a vector database from these embeddings, allowing for efficient similarity search.

### 5. Create a Retriever
```python
retriever = vectordb.as_retriever()
```
Turns the vector database into a retriever object, which can fetch relevant documents based on a query.

### 6. Initialize the Language Model (LLM)
```python
llm = ChatOpenAI(model="gpt-3.5-turbo", api_key=api_key)
```
Initializes the OpenAI chat model (e.g., GPT-3.5 Turbo) for generating answers.

### 7. Set Up the Retrieval QA Pipeline
```python
qa = RetrievalQA.from_chain_type(llm, retriever=retriever)
```
Combines the retriever and the LLM into a RetrievalQA pipeline. This allows the model to search for relevant documents and use them to answer questions.

### 8. Run a Query
```python
answer = qa.run("What is FastAPI?")
print(answer)
```
- Runs a sample question through the pipeline.
- Prints the answer generated by the LLM, using the retrieved context.

---

## How to Run
1. Clone this repository.
2. Install dependencies:
	 ```sh
	 pip install -r requirements.txt
	 ```
3. Create a `.env` file with your OpenAI API key:
	 ```env
	 OPENAI_API_KEY=your_openai_key_here
	 ```
4. Run the script:
	 ```sh
	 python main.py
	 ```

---

## Future Possibilities

- **S3 for Document Storage:**
	- Store and load large document dumps from AWS S3 buckets for scalable data ingestion.
- **Document Chunking:**
	- Split large documents into smaller, manageable chunks for better retrieval and context handling.
- **AWS Lambda Functions:**
	- Wrap this logic in Python or Go Lambda functions for serverless, scalable deployments on AWS.
- **Vector DB in DynamoDB:**
	- Store vector embeddings in DynamoDB for persistent, cloud-native vector search.

---

## License
See [LICENSE](LICENSE) for details.
